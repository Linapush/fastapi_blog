# FastAPI проект

pool_recycle - через сколько что-то перезагружать, обращаться с запросом еще раз

**jwt token** - три зоны(header, payload, signature). Можно расшифровать если есть secret key (называется secret salt)

signature отвечает за расшифровку

два вида: access token, refresh token

На фронт отправляем токен, а на бэке его можно расшифровать

expire - время, которое токен живет (для access token default 15 min)

Зашифровать можно не строку, а {’a’: ‘b’} например

refresh token - expire = 1 day

Храним его в бд. Будет login-ручка и refresh-ручка.

**kafka**.msgpack - стандантизированный метод передачи байтов

В бд байты не хранятся.

**flake8** .

isort .

black .

Эти два сами там как-то форматируют код. Как ctrl+shift+i, только лучше.

mypy .

---

13.12

Метод cast модуля typing - ему передается, какой тип будет возвращаться откуда

Это мы делаем вроде, чтобы pycharm подсказывал.

Библиотека orjson - как json, только быстрее. Методы load и dump.

В fastapi.responses есть ORJSONresponse. Так делать грамотно.

Для каждой схемы в моделях отдельная папка. В meta.py прописываем ее название.

stem - начальное название файла без его суффикса (без типа файла)

orgjson нужен в fastAPI, здесь в моделях не нужен. Он нужен в production.

**Модели**: каждая модель создается в своем файле.

---

14.12

**kafka**

- аналог reddis (у него под капотом создается linked list, в который вы кладете и из него забираете) и rabbit.

kafka - надежный формат сообщений. Создан для высоконагруженных систем, чтобы много данных туда-сюда гонять в очереди.

Producer кладет в очередь, consumer читает с очереди.

Внутри kafka есть большие очереди (топики), у каждой свое название. Внутри топика еще партиции. Producer приходит и кладет в партицию. Consumerов можно поднять несколько на один топик, тогда они распредены между партициями. Если consumer упал, получается из этой партиции читать не будет. Но consumer может читать только из одного топика. Количество consumerов ≥  количеству топиков.

x.append()   x.pop()   - так другой не сможет прочитать те же данные.

У kafka не так. Тут есть время жизни сообщений. У worker есть свой offset, то есть сдвиг. Он в следующий раз может попросить данные уже с 10 позиции.

**Проект дальше**

msgpack - двоичная форма представления простых структур данных, таких как массивы и ассоциативные массивы. *******************It’s like JSON, but fast and small.*******************

kafka добавляем в docker-compose.yml

`producer: AIOFafkaProducer` в файле db/kafka.py - это анотация, его еще нет, но он может быть. В on_startup/kafka.py мы как раз его создаем.

---

16.12

**Тесты**

monkeypatch - подменяет что-то чем-то

Функция в тестах начинается с _, если она ничего не возвращает.

---

19.12

Мы продолжаем тесты.

В тестах нет kafka. Вместо того, чтобы брать встроенный AIOKafkaProducer, мы будем создавать класс TestKafkaProducer.

a = MagicMock spec = …    - пишем методы, которые есть у объекта a.

**Метрики**

*Метрика интерфейса — это совокупность инструментов и методов для оценки и оптимизации пользовательского интерфейса веб-сайта.*

Метрики – это сразу общая картина о состоянии приложения. Мы собираем не все детали, а только готовую выжимку: например, количество запросов к сервису.

DevOps - это админ + бекендер

[Человеческим языком про метрики 1: Потерянное введение](https://habr.com/ru/companies/tochka/articles/683608/)

Будем использовать prometheus и grafana. Берем дефолтные контейнеры.

Методы тоже дефолтные. Можно сходить на [localhost:8000/metrics](http://localhost:8000/metrics) и посмотреть, что возвращает.

Нам понадобятся методы Counter и Histogram для зачета.

Мы че-то там в postman на auth/… (или на file/resize еще можно) отправляем, какой-то токен куда-то пишем…

Потом смотрим на /metrics, что-то добавилось. Вот эти le - это про бакеты (отрезок, который он на доске рисовал).

На порте 3000 лежит grafana.

Через декоратор где у него написано - можно не делать.

“Придумайте свой декоратор на собеседовании.” Чего….

---

Запуск проекта: `docker compose up --build`

Устанавливать вроде ничего не нужно, оно есть в requirements (?)

`curl … -o temp.png`  - to resize image. Что-то у нас не работает.